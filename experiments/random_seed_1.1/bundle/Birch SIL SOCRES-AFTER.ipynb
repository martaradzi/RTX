{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marta/env/lib/python3.6/site-packages/scikit_learn-0.22.2.post1-py3.6-linux-x86_64.egg/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import plot, show, savefig, xlim, figure,  ylim, legend, boxplot, setp, axes\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import collections\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "\n",
    "def get_cal_score(data, labels):\n",
    "    return metrics.calinski_harabasz_score(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'totalCarNumber',\n",
       " 'numberOfTrips',\n",
       " 'median_overhead',\n",
       " 'q1_overhead',\n",
       " 'q3_overhead',\n",
       " 'p9_overhead']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(df.columns)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marta/env/lib/python3.6/site-packages/pandas-1.0.3-py3.6-linux-x86_64.egg/pandas/core/indexing.py:845: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/marta/env/lib/python3.6/site-packages/pandas-1.0.3-py3.6-linux-x86_64.egg/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/home/marta/env/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_before =df.iloc[144:, :]\n",
    "\n",
    "\n",
    "indexes = np.array_split(df_before.index,8, axis=0)\n",
    "for i,index in enumerate(indexes):\n",
    "    df_before.loc[index,'group'] = i\n",
    "    \n",
    "df_before['c'] = df_before['group'].diff()\n",
    "df_filtered = df_before[df_before['c'] != 0]\n",
    "\n",
    "index_list = df_filtered.index.tolist() # list of the start poisitions of index for change of values \n",
    "# print(len(index_list))\n",
    "# print('\\n')\n",
    "# print(index_list)\n",
    "\n",
    "l_mod = index_list + [max(index_list)+1] # creating a list of indexes to iterate over (must have 0 in it)\n",
    "list_of_dfs = [df_before.iloc[l_mod[n]:l_mod[n+1]] for n in range(len(l_mod)-1)] # creating a list of dfs for each index\n",
    "\n",
    "# len(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette_scores(model, test_data, n_clusters_min, n_clusters_max, title):\n",
    "    \"\"\" Plot silhouette scores and return the best number of clusters\"\"\"\n",
    "\n",
    "    if len(model.subcluster_labels_) >= 2:\n",
    "\n",
    "        silhouette_scores = []\n",
    "        clusters_range = range(n_clusters_min, n_clusters_max+1)\n",
    "        results_dict = []\n",
    "        for number in clusters_range:\n",
    "            # make a copy of the model so as not to mess up the 'correct' model\n",
    "            model_cpy = copy.deepcopy(model)\n",
    "            model_cpy.set_params(n_clusters=number)\n",
    "\n",
    "            model_cpy.partial_fit()\n",
    "            labels = model_cpy.predict(test_data)\n",
    "            try: \n",
    "                s = metrics.silhouette_score(test_data, labels, metric='euclidean')\n",
    "                silhouette_scores.append(s)\n",
    "                results_dict.append((number, s))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        silhouette_range = [i[0] for i in results_dict]  \n",
    "        max_score = max(silhouette_scores)\n",
    "        for i in results_dict:\n",
    "            if i[1] == max_score:\n",
    "                print(max_score)\n",
    "                return int(i[0])\n",
    "    else:\n",
    "        print(0)\n",
    "        return n_clusters_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dfs(df_list):\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_removal(df):\n",
    "    data_split_modified_z = df[['median_overhead', 'q1_overhead', 'q3_overhead', 'p9_overhead', 'totalCarNumber', 'index']].copy()\n",
    "    # to_drop = [252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,265,266]\n",
    "    # data_split_modified_z.drop(to_drop, inplace=True)\n",
    "#     data_split_modified_z = data_split_modified_z.iloc[:144, :]\n",
    "    data_split_modified_z = data_split_modified_z.to_numpy()\n",
    "    data_split_modified_z_12 = np.split(data_split_modified_z, 1)\n",
    "    indexes = [] \n",
    "\n",
    "    for d in range(len(data_split_modified_z_12)):\n",
    "        ind = []\n",
    "        k = len(data_split_modified_z_12[d])\n",
    "        for i in [0, 1, 2, 3]:\n",
    "            ys = data_split_modified_z_12[d][:,i] \n",
    "            median_y = np.median(ys)\n",
    "            median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in ys])\n",
    "    #         print(median_absolute_deviation_y)\n",
    "            for y in range(len(data_split_modified_z_12[d])):\n",
    "                modified_z_score = 0.6745 * (data_split_modified_z_12[d][y, i] - median_y) / median_absolute_deviation_y\n",
    "    #             z_score = (data_split[d][y, i]  - mean_ys) / std_ys\n",
    "                if np.abs(modified_z_score) > 3.5:\n",
    "                    ind.append(y)\n",
    "\n",
    "\n",
    "        data_split_modified_z_12[d] = np.delete(data_split_modified_z_12[d], ind, axis=0)\n",
    "#         print('Outliers detected: ' + str((k - len(data_split_modified_z_12[d]))))\n",
    "\n",
    "    data_split_modified_z_12 = np.concatenate(data_split_modified_z_12, axis=0)\n",
    "    return data_split_modified_z_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a day of data (no accident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marta/env/lib/python3.6/site-packages/numpy-1.18.2-py3.6-linux-x86_64.egg/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/marta/env/lib/python3.6/site-packages/numpy-1.18.2-py3.6-linux-x86_64.egg/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-13fd816659d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnumpy_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutliers_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel_001_before\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_001_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/scikit_learn-0.22.2.post1-py3.6-linux-x86_64.egg/sklearn/cluster/_birch.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/scikit_learn-0.22.2.post1-py3.6-linux-x86_64.egg/sklearn/cluster/_birch.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mbranching_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranching_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/scikit_learn-0.22.2.post1-py3.6-linux-x86_64.egg/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    584\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 586\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "model_001_before = Birch(n_clusters=None, threshold=0.01)\n",
    "\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "removed_outliers_numpy = []\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "labels_dict = {}\n",
    "label_1_dict = {}\n",
    "label_0_dict = {}\n",
    "add_data_to_this = pd.DataFrame(columns = columns)\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "list_of_dfs\n",
    "\n",
    "for i in range(0, len(list_of_dfs)):\n",
    "    numpy_array = outliers_removal(list_of_dfs[i])\n",
    "    \n",
    "    model_001_before.partial_fit(numpy_array[:, :-2])\n",
    "    \n",
    "    model_copy = copy.deepcopy(model_001_before)\n",
    "    \n",
    "    removed_outliers_numpy.append(numpy_array)\n",
    "    new_nmpy = np.concatenate(removed_outliers_numpy, axis=0)\n",
    "    \n",
    "    n = plot_silhouette_scores(model_copy, new_nmpy[:, :-2], 2, 10, fit_number)\n",
    "#     print(n)\n",
    "    \n",
    "    model_copy.set_params(n_clusters = n)\n",
    "    model_copy.partial_fit()\n",
    "    \n",
    "    labels_dict[f'fit_{fit_number}'] = model_copy.predict(new_nmpy[:, :-2])\n",
    "    unique, counts = np.unique(labels_dict[f'fit_{fit_number}'], return_counts=True)\n",
    "    label_1_dict[f'fit_{fit_number}'] = counts[1]\n",
    "    label_0_dict[f'fit_{fit_number}'] = counts[0]\n",
    "#     print(collections.Counter(labels_dict[f'fit_{fit_number}']))\n",
    "#     print(get_cal_score(new_nmpy[:, :-2], labels_dict[f'fit_{fit_number}']))\n",
    "    list_of_nmpys_for_graphs.append(new_nmpy)\n",
    "    fit_number += 1\n",
    "    \n",
    "for i in label_0_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in label_1_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "col_counter = 0\n",
    "row_counter = 0\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "figure, axs = plt.subplots(nrows=nrows, ncols=ncols,figsize=(14,14))\n",
    "\n",
    "for i in list_of_nmpys_for_graphs:\n",
    "    \n",
    "#     numpy_array = i.to_numpy()\n",
    "    \n",
    "    figure.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    axs[row_counter,col_counter].scatter(i[:, 5], i[:, 4], c =labels_dict[f'fit_{fit_number}'], cmap='rainbow', alpha=0.7 )\n",
    "    axs[row_counter,col_counter].set_xlabel('Time of the day')\n",
    "    axs[row_counter,col_counter].set_ylim(0, 750)\n",
    "    axs[row_counter,col_counter].set_ylabel('Car number')\n",
    "    axs[row_counter,col_counter].set_title(f'Fit {fit_number}')\n",
    "    \n",
    "    if fit_number >= 9:\n",
    "        axs[row_counter,col_counter].axvline(x=143, color='g')\n",
    "    if col_counter == (ncols-1):\n",
    "        col_counter = 0\n",
    "        row_counter += 1\n",
    "    else:\n",
    "        col_counter += 1\n",
    "    \n",
    "    fit_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_005_before = Birch(n_clusters=None, threshold=0.05)\n",
    "\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "removed_outliers_numpy = []\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "labels_dict = {}\n",
    "label_1_dict = {}\n",
    "label_0_dict = {}\n",
    "add_data_to_this = pd.DataFrame(columns = columns)\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "list_of_dfs\n",
    "\n",
    "for i in range(0, len(list_of_dfs)):\n",
    "    numpy_array = outliers_removal(list_of_dfs[i])\n",
    "    \n",
    "    model_005_before.partial_fit(numpy_array[:, :-2])\n",
    "    \n",
    "    model_copy = copy.deepcopy(model_005_before)\n",
    "    \n",
    "    removed_outliers_numpy.append(numpy_array)\n",
    "    new_nmpy = np.concatenate(removed_outliers_numpy, axis=0)\n",
    "    \n",
    "    n = plot_silhouette_scores(model_copy, new_nmpy[:, :-2], 2, 10, fit_number)\n",
    "#     print(n)\n",
    "    \n",
    "    model_copy.set_params(n_clusters = n)\n",
    "    model_copy.partial_fit()\n",
    "    \n",
    "    labels_dict[f'fit_{fit_number}'] = model_copy.predict(new_nmpy[:, :-2])\n",
    "    unique, counts = np.unique(labels_dict[f'fit_{fit_number}'], return_counts=True)\n",
    "    label_1_dict[f'fit_{fit_number}'] = counts[1]\n",
    "    label_0_dict[f'fit_{fit_number}'] = counts[0]\n",
    "#     print(collections.Counter(labels_dict[f'fit_{fit_number}']))\n",
    "#     print(get_cal_score(new_nmpy[:, :-2], labels_dict[f'fit_{fit_number}']))\n",
    "    list_of_nmpys_for_graphs.append(new_nmpy)\n",
    "    fit_number += 1\n",
    "    \n",
    "print('\\n')    \n",
    "for i in label_0_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in label_1_dict.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1_before = Birch(n_clusters=None, threshold=0.1)\n",
    "\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "removed_outliers_numpy = []\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "labels_dict = {}\n",
    "label_1_dict = {}\n",
    "label_0_dict = {}\n",
    "add_data_to_this = pd.DataFrame(columns = columns)\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "list_of_dfs\n",
    "\n",
    "for i in range(0, len(list_of_dfs)):\n",
    "    numpy_array = outliers_removal(list_of_dfs[i])\n",
    "    \n",
    "    model_1_before.partial_fit(numpy_array[:, :-2])\n",
    "    \n",
    "    model_copy = copy.deepcopy(model_1_before)\n",
    "    \n",
    "    removed_outliers_numpy.append(numpy_array)\n",
    "    new_nmpy = np.concatenate(removed_outliers_numpy, axis=0)\n",
    "    \n",
    "    n = plot_silhouette_scores(model_copy, new_nmpy[:, :-2], 2, 10, fit_number)\n",
    "#     print(n)\n",
    "    \n",
    "    model_copy.set_params(n_clusters = n)\n",
    "    model_copy.partial_fit()\n",
    "    \n",
    "    labels_dict[f'fit_{fit_number}'] = model_copy.predict(new_nmpy[:, :-2])\n",
    "    unique, counts = np.unique(labels_dict[f'fit_{fit_number}'], return_counts=True)\n",
    "    label_1_dict[f'fit_{fit_number}'] = counts[1]\n",
    "    label_0_dict[f'fit_{fit_number}'] = counts[0]\n",
    "#     print(collections.Counter(labels_dict[f'fit_{fit_number}']))\n",
    "#     print(get_cal_score(new_nmpy[:, :-2], labels_dict[f'fit_{fit_number}']))\n",
    "    list_of_nmpys_for_graphs.append(new_nmpy)\n",
    "    fit_number += 1\n",
    "print('\\n')\n",
    "\n",
    "for i in label_0_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in label_1_dict.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_02_before = Birch(n_clusters=None, threshold=0.2)\n",
    "\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "removed_outliers_numpy = []\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "labels_dict = {}\n",
    "label_1_dict = {}\n",
    "label_0_dict = {}\n",
    "add_data_to_this = pd.DataFrame(columns = columns)\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "list_of_dfs\n",
    "\n",
    "for i in range(0, len(list_of_dfs)):\n",
    "    numpy_array = outliers_removal(list_of_dfs[i])\n",
    "    \n",
    "    model_02_before.partial_fit(numpy_array[:, :-2])\n",
    "    \n",
    "    model_copy = copy.deepcopy(model_02_before)\n",
    "    \n",
    "    removed_outliers_numpy.append(numpy_array)\n",
    "    new_nmpy = np.concatenate(removed_outliers_numpy, axis=0)\n",
    "    \n",
    "    n = plot_silhouette_scores(model_copy, new_nmpy[:, :-2], 2, 10, fit_number)\n",
    "#     print(n)\n",
    "    \n",
    "    model_copy.set_params(n_clusters = n)\n",
    "    model_copy.partial_fit()\n",
    "    \n",
    "    labels_dict[f'fit_{fit_number}'] = model_copy.predict(new_nmpy[:, :-2])\n",
    "    unique, counts = np.unique(labels_dict[f'fit_{fit_number}'], return_counts=True)\n",
    "    label_1_dict[f'fit_{fit_number}'] = counts[1]\n",
    "    label_0_dict[f'fit_{fit_number}'] = counts[0]\n",
    "#     print(collections.Counter(labels_dict[f'fit_{fit_number}']))\n",
    "#     print(get_cal_score(new_nmpy[:, :-2], labels_dict[f'fit_{fit_number}']))\n",
    "    list_of_nmpys_for_graphs.append(new_nmpy)\n",
    "    fit_number += 1\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "for i in label_0_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in label_1_dict.values():\n",
    "    print(i)\n",
    "\n",
    "col_counter = 0\n",
    "row_counter = 0\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "figure, axs = plt.subplots(nrows=nrows, ncols=ncols,figsize=(14,14))\n",
    "\n",
    "for i in list_of_nmpys_for_graphs:\n",
    "    \n",
    "#     numpy_array = i.to_numpy()\n",
    "    \n",
    "    figure.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    axs[row_counter,col_counter].scatter(i[:, 5], i[:, 4], c =labels_dict[f'fit_{fit_number}'], cmap='rainbow', alpha=0.7 )\n",
    "    axs[row_counter,col_counter].set_xlabel('Time of the day')\n",
    "    axs[row_counter,col_counter].set_ylim(0, 750)\n",
    "    axs[row_counter,col_counter].set_ylabel('Car number')\n",
    "    axs[row_counter,col_counter].set_title(f'Fit {fit_number}')\n",
    "    \n",
    "    if fit_number >= 9:\n",
    "        axs[row_counter,col_counter].axvline(x=143, color='g')\n",
    "    if col_counter == (ncols-1):\n",
    "        col_counter = 0\n",
    "        row_counter += 1\n",
    "    else:\n",
    "        col_counter += 1\n",
    "    \n",
    "    fit_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_3_before = Birch(n_clusters=None, threshold=0.3)\n",
    "\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "removed_outliers_numpy = []\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "labels_dict = {}\n",
    "label_1_dict = {}\n",
    "label_0_dict = {}\n",
    "add_data_to_this = pd.DataFrame(columns = columns)\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "list_of_dfs\n",
    "\n",
    "for i in range(0, len(list_of_dfs)):\n",
    "    numpy_array = outliers_removal(list_of_dfs[i])\n",
    "    \n",
    "    model_3_before.partial_fit(numpy_array[:, :-2])\n",
    "    \n",
    "    model_copy = copy.deepcopy(model_3_before)\n",
    "    \n",
    "    removed_outliers_numpy.append(numpy_array)\n",
    "    new_nmpy = np.concatenate(removed_outliers_numpy, axis=0)\n",
    "    \n",
    "    n = plot_silhouette_scores(model_copy, new_nmpy[:, :-2], 2, 10, fit_number)\n",
    "#     print(n)\n",
    "    \n",
    "    model_copy.set_params(n_clusters = n)\n",
    "    model_copy.partial_fit()\n",
    "    \n",
    "    labels_dict[f'fit_{fit_number}'] = model_copy.predict(new_nmpy[:, :-2])\n",
    "    unique, counts = np.unique(labels_dict[f'fit_{fit_number}'], return_counts=True)\n",
    "    try:\n",
    "        label_1_dict[f'fit_{fit_number}'] = counts[1]\n",
    "    except:\n",
    "        label_1_dict[f'fit_{fit_number}'] = 0\n",
    "    label_0_dict[f'fit_{fit_number}'] = counts[0]\n",
    "#     print(collections.Counter(labels_dict[f'fit_{fit_number}']))\n",
    "#     print(get_cal_score(new_nmpy[:, :-2], labels_dict[f'fit_{fit_number}']))\n",
    "    list_of_nmpys_for_graphs.append(new_nmpy)\n",
    "    fit_number += 1\n",
    "    \n",
    "print('\\n')\n",
    "for i in label_0_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in label_1_dict.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_5_before = Birch(n_clusters=None, threshold=0.5)\n",
    "\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "removed_outliers_numpy = []\n",
    "list_of_nmpys_for_graphs = []\n",
    "\n",
    "labels_dict = {}\n",
    "label_1_dict = {}\n",
    "label_0_dict = {}\n",
    "add_data_to_this = pd.DataFrame(columns = columns)\n",
    "\n",
    "fit_number = 1\n",
    "\n",
    "list_of_dfs\n",
    "\n",
    "for i in range(0, len(list_of_dfs)):\n",
    "    numpy_array = outliers_removal(list_of_dfs[i])\n",
    "    \n",
    "    model_5_before.partial_fit(numpy_array[:, :-2])\n",
    "    \n",
    "    model_copy = copy.deepcopy(model_5_before)\n",
    "    \n",
    "    removed_outliers_numpy.append(numpy_array)\n",
    "    new_nmpy = np.concatenate(removed_outliers_numpy, axis=0)\n",
    "    \n",
    "    n = plot_silhouette_scores(model_copy, new_nmpy[:, :-2], 2, 10, fit_number)\n",
    "#     print(n)\n",
    "    \n",
    "    model_copy.set_params(n_clusters = n)\n",
    "    model_copy.partial_fit()\n",
    "    \n",
    "    labels_dict[f'fit_{fit_number}'] = model_copy.predict(new_nmpy[:, :-2])\n",
    "    unique, counts = np.unique(labels_dict[f'fit_{fit_number}'], return_counts=True)\n",
    "    try:\n",
    "        label_1_dict[f'fit_{fit_number}'] = counts[1]\n",
    "    except:\n",
    "        label_1_dict[f'fit_{fit_number}'] = 0\n",
    "    label_0_dict[f'fit_{fit_number}'] = counts[0]\n",
    "#     print(collections.Counter(labels_dict[f'fit_{fit_number}']))\n",
    "#     print(get_cal_score(new_nmpy[:, :-2], labels_dict[f'fit_{fit_number}']))\n",
    "    list_of_nmpys_for_graphs.append(new_nmpy)\n",
    "    fit_number += 1\n",
    "\n",
    "print('\\n')\n",
    "for i in label_0_dict.values():\n",
    "    print(i)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in label_1_dict.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
